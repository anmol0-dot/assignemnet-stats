# üç∑ Wine Dataset Principal Component Analysis (PCA)

This document provides a summary of the Principal Component Analysis (PCA) performed on the well-known **Wine dataset**.  
PCA is a dimensionality reduction technique that transforms high-dimensional data into fewer uncorrelated components while retaining maximum variance.

---

## üõ†Ô∏è Analysis Steps

The analysis reduces the dataset from **13 features** to **2 principal components (PC1 and PC2)**.

---

### **1. Data Loading and Exploration**

The Wine dataset contains:
- **13 chemical features** measured from wine samples  
- **3 target classes** representing different cultivars (0, 1, and 2)

A key observation during exploration:
- Features had **large differences in scale**, e.g.  
  - *Proline* has a mean around **747**  
  - *Hue* has a mean around **0.96**

This scale imbalance makes preprocessing essential.

---

### **2. Feature Scaling**

PCA is sensitive to the magnitude of features.  
So, all features were standardized using **StandardScaler**, which transforms data such that:

- Mean = **0**  
- Standard deviation = **1**

This ensures that all features contribute equally to PCA.

---

### **3. Principal Component Analysis**

PCA was applied to the scaled data with:

The resulting components:
- **PC1** and **PC2**  
- Capture the maximum variance in the dataset  
- Represent new uncorrelated axes for visualization and analysis  

---

### **4. Visualization**

A scatter plot of PC1 vs PC2 was created, where:
- Each point represents a wine sample  
- Colors represent the original target classes (0, 1, 2)

---

## ‚úÖ Conclusion

PCA proved highly effective in summarizing the complex Wine dataset.

### **Key Findings**

#### ‚úîÔ∏è Excellent Separability  
The three wine classes form **distinct clusters** in the 2-D PCA space.

#### ‚úîÔ∏è Strong Dimensionality Reduction  
The first two principal components retain the **majority of the variance** required to differentiate wine types.

#### ‚úîÔ∏è Modeling Advantage  
Because PC1 and PC2 separate classes so well:
- The original **13 features are largely redundant**
- Data can be simplified **without losing critical information**
- PCA-transformed data is excellent for **classification models**

---

This PCA demonstrates how high-dimensional data can be visualized and understood using only two features, making it a powerful tool in exploratory data analysis and machine learning workflows.
